# ==============================================================================
# UNIFIED NET2NET CONFIGURATION
# One file for both "Stage 1 (Small)" and "Stage 2 (Expansion)"
# ==============================================================================

# ------------------------------------------------------------------------------
# HOW TO RUN:
#
# 1. FOR STAGE 1 (Start Small):
#    python TensorJAX_UniformNet2Net.py model.layers=[9,64,64,6] transfer_checkpoint=null stage_tag="1.0_1.8"
#
# 2. FOR STAGE 2 (Expand & Conquer):
#    python TensorJAX_UniformNet2Net.py transfer_checkpoint="path/to/stage1/best_checkpoint.msgpack"
# ------------------------------------------------------------------------------

# --- GLOBAL SETTINGS ---
seed: 42
mode: multi_stage       # "single_stage" allows precise control per step. Use "multi_stage" for loops.
stage_tag: "1.0_2.4"     # Default focus on the HARD stages (Stage 2)
n_samples: 10000
model_type: maxwell_B

# --- NET2NET TRIGGER (The Magic Switch) ---
# If you provide a path here, AND the loaded weights are smaller than 'model.layers' below,
# the code will AUTOMATICALLY trigger the Net2Net expansion (Zero-Padding).
transfer_checkpoint: null  # Default: null (Scratch). Override via command line for Stage 2.

# --- PHYSICS PARAMETERS ---
eta0: 1.0
lam: 0.5
lam_r: 1.0

# --- DATA SETTINGS ---
data:
  scaling_mode: standard
  replay_ratio: 0.2      # Keeps 20% of Stage 1 data to prevent forgetting

# --- MODEL ARCHITECTURE (The TARGET / BIG Brain) ---
model:
  # We define the FINAL desired architecture here (e.g., 3 layers of 512).
  # If running Stage 1, override this to [9, 64, 64, 6] on the command line.
  layers: [9, 512, 512, 512, 6]  
  
  dropout: 0.0
  activation: tanh       # Tanh is critical for function preservation

# --- TRAINING HYPERPARAMETERS ---
training:
  lambda_phys: 1.0       # Physics constraint strength
  num_epochs: 2000       # Give the big model enough time to converge
  batch_size: 128
  learning_rate: 1e-4
  weight_decay: 1e-5